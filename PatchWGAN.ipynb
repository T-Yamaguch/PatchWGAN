{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PatchWGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Yamaguch/PatchWGAN/blob/master/PatchWGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Tzp4G4_cKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e327b2c0-dc96-4022-c55f-87bcea50c13a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkBM0L4_c9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, \\\n",
        "MaxPooling2D, Activation, ReLU, LeakyReLU, UpSampling2D, BatchNormalization, \\\n",
        "Dropout, Dense, Flatten, Add, LayerNormalization, GaussianNoise, Reshape, Lambda\n",
        "from keras.regularizers import l2\n",
        "\n",
        "class conv_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.up = UpSampling2D((2,2))\n",
        "    self.noise = GaussianNoise(0.2)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.up(x)\n",
        "    x = self.noise(x)\n",
        "    return x\n",
        "\n",
        "class res_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(res_block, self).__init__()\n",
        "    self.conv1 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.conv2 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm1 = BatchNormalization(trainable=True)\n",
        "    self.norm2 = BatchNormalization(trainable=True)\n",
        "    self.act1 = LeakyReLU()\n",
        "    self.act2 = LeakyReLU()\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.conv1(x)\n",
        "    y = self.norm1(y)\n",
        "    y = self.act1(y)\n",
        "    y = self.conv2(y)\n",
        "    y = self.norm2(y)\n",
        "    y = self.act2(y)\n",
        "    x = self.add([x, y])\n",
        "    return x\n",
        "\n",
        "class disc_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(disc_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    # x = self.norm(x) dにnorm入れないほうがいいという噂\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class dense_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(dense_block, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "class dense_block_wo_norm(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(dense_block_wo_norm, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.act = LeakyReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2s6XE9_mkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "0f29dca6-1b01-489a-9e67-adccdc631e60"
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 2048\n",
        "    self.layer_num = 4\n",
        "    self.res_num = 0\n",
        "    self.latent_num = 8\n",
        "    self.inputs = Input(shape=(self.latent_num)) \n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'generator'\n",
        "    self.kernel_regularizer= None\n",
        "      \n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    final_size = 4*4*self.channel_num\n",
        "    data_size = self.latent_num\n",
        "\n",
        "    while data_size*64 < final_size:\n",
        "      data_size *= 64\n",
        "      x = dense_block(data_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = dense_block(final_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Reshape((4, 4, self.channel_num))(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    \n",
        "    for n in range(self.layer_num):\n",
        "      for m in range(self.res_num):\n",
        "        x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num /= 2\n",
        "      x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    for m in range(self.res_num):\n",
        "      x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = Conv2D(3, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = x\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "g = Generator()\n",
        "g.model().summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_block (dense_block)    (None, 512)               6656      \n",
            "_________________________________________________________________\n",
            "dense_block_1 (dense_block)  (None, 32768)             16941056  \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 2048)        0         \n",
            "_________________________________________________________________\n",
            "conv_block (conv_block)      (None, 8, 8, 1024)        52433920  \n",
            "_________________________________________________________________\n",
            "conv_block_1 (conv_block)    (None, 16, 16, 512)       13109760  \n",
            "_________________________________________________________________\n",
            "conv_block_2 (conv_block)    (None, 32, 32, 256)       3278080   \n",
            "_________________________________________________________________\n",
            "conv_block_3 (conv_block)    (None, 64, 64, 128)       819840    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 3)         9603      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 64, 64, 3)         0         \n",
            "=================================================================\n",
            "Total params: 86,598,915\n",
            "Trainable params: 86,528,515\n",
            "Non-trainable params: 70,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie8PcTw_nEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "ad8298fe-010f-419f-ff59-e5d693cd3acb"
      },
      "source": [
        "class Discriminator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 16\n",
        "    self.layer_num = 3\n",
        "    self.latent_num = 8\n",
        "    self.input_shape = (64, 64, 3)\n",
        "    self.inputs = Input(shape=self.input_shape)\n",
        "    self.kernel_size = (4, 4)\n",
        "    self.name = 'discriminator'\n",
        "    self.kernel_regularizer= None\n",
        "\n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      x = disc_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "\n",
        "    y = Conv2D(filter_num, self.kernel_size, padding='same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    y = LeakyReLU()(y)\n",
        "    y = Conv2D(1, self.kernel_size, padding='same', kernel_regularizer= self.kernel_regularizer)(y)\n",
        "    y = Flatten()(y)\n",
        "    y = Dense(1)(y)\n",
        "\n",
        "\n",
        "    while x.shape[1] != 4:\n",
        "      x = disc_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "    x = Flatten()(x)\n",
        "    x = dense_block_wo_norm(64, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Dense(1)(x)\n",
        "    \n",
        "    outputs = Add()([x, y])\n",
        "\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "d = Discriminator()\n",
        "d.model().summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "disc_block (disc_block)         (None, 32, 32, 16)   784         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "disc_block_1 (disc_block)       (None, 16, 16, 32)   8224        disc_block[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "disc_block_2 (disc_block)       (None, 8, 8, 64)     32832       disc_block_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 128)    131200      disc_block_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "disc_block_3 (disc_block)       (None, 4, 4, 128)    131200      disc_block_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 128)    0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           disc_block_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 1)      2049        leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_block_wo_norm (dense_bloc (None, 64)           131136      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            65          dense_block_wo_norm[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            65          flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 1)            0           dense_4[0][0]                    \n",
            "                                                                 dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 437,555\n",
            "Trainable params: 437,555\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTiix-n_pgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71eb0c09-e4cb-40b0-f96b-4d1e4f8a8419"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import binary_crossentropy, MSE\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "\n",
        "class WGAN():\n",
        "  def __init__(self, \n",
        "               img_size=128, \n",
        "               code_num = 2048,\n",
        "               batch_size = 16, \n",
        "               train_epochs = 100, \n",
        "               train_steps = 8, \n",
        "               checkpoint_epochs = 25, \n",
        "               image_epochs = 1, \n",
        "               start_epoch = 1,\n",
        "               optimizer = Adam(learning_rate = 1e-4),\n",
        "               n_critics = 8\n",
        "               ):\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.train_epochs =  train_epochs\n",
        "    self.train_steps = train_steps\n",
        "    self.checkpoint_epochs = checkpoint_epochs\n",
        "    self.image_epochs = image_epochs\n",
        "    self.start_epoch = start_epoch\n",
        "    self.code_num = code_num\n",
        "    self.img_size = img_size\n",
        "    self.n_critics = n_critics\n",
        "    \n",
        "    self.gen_optimizer = optimizer\n",
        "    self.disc_optimizer = optimizer\n",
        "\n",
        "    g = Generator()\n",
        "    self.gen = g.model()\n",
        "    \n",
        "    d = Discriminator()\n",
        "    self.disc = d.model()\n",
        "\n",
        "    checkpoint_dir = \"drive/My Drive/PatchWGAN/checkpoint\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(gen_optimizer = self.gen_optimizer,\n",
        "                                     disc_optimizer = self.disc_optimizer,\n",
        "                                     gen = self.gen,\n",
        "                                     disc = self.disc,\n",
        "                                     )\n",
        "\n",
        "    self.manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "    train_image_path = 'drive/My Drive/samples/image'\n",
        "    \n",
        "    self.train_filenames = glob.glob(train_image_path + '/*.jpg') \n",
        "\n",
        "    checkpoint.restore(self.manager.latest_checkpoint)\n",
        "\n",
        "    self.g_history = []\n",
        "    self.d_history = []\n",
        "    # self.endec_history = []  \n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [self.img_size, self.img_size] )\n",
        "    image = image/255  # normalize to [0,1] range\n",
        "    return tf.cast(image, tf.float32)\n",
        "\n",
        "  def load_and_preprocess_image(self, path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return self.preprocess_image(image)\n",
        "\n",
        "  def dataset(self, paths, batch_size):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    img_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    img_ds = img_ds.batch(batch_size)\n",
        "    return img_ds\n",
        "\n",
        "  def image_preparation(self, filenames, batch_size, steps):\n",
        "    img_batch = []\n",
        "    while 1:\n",
        "      random.shuffle(filenames)\n",
        "      for path in filenames:\n",
        "        img_batch.append(path)\n",
        "        if len(img_batch) == steps*batch_size:\n",
        "          imgs = self.dataset(img_batch, batch_size)\n",
        "          img_batch = []\n",
        "          yield imgs\n",
        "\n",
        "  def discriminator_loss(self, original_outputs, generated_outputs):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(original_outputs), original_outputs)\n",
        "    generated_loss = binary_crossentropy(tf.zeros_like(generated_outputs), generated_outputs)\n",
        "    loss_d = tf.math.reduce_mean(real_loss + generated_loss)\n",
        "    return loss_d\n",
        "\n",
        "  def generator_loss(self, generated_outputs):\n",
        "    loss_g = tf.math.reduce_mean(binary_crossentropy(tf.ones_like(generated_outputs), generated_outputs))\n",
        "    return loss_g\n",
        "\n",
        "  def mse_loss(self, true, pred):\n",
        "    loss =  tf.math.reduce_mean(MSE(true, pred))\n",
        "    return loss\n",
        "\n",
        "  def wasserstein_loss(self, ori_outputs, gen_outputs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  def g_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=False)\n",
        "      gen_outputs = self.disc(gen_imgs, training=False)\n",
        "\n",
        "      _, g_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_temp.append(g_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "  def d_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=False)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      d_loss, _ = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def visualise_batch(self, s_1, epoch):\n",
        "    gen_img = self.gen(s_1)  \n",
        "    gen_img = (np.array(gen_img*255, np.uint8))\n",
        "    fig, axes = plt.subplots(4, 6)\n",
        "    for idx, img in enumerate(gen_img):\n",
        "      p, q = idx//6, idx%6\n",
        "      axes[p, q].imshow(img)\n",
        "      axes[p, q].axis('off')\n",
        "    \n",
        "    save_name = 'drive/My Drive/PatchWGAN/generated_image/'+'image_at_epoch_{:04d}.png'\n",
        "    plt.savefig(save_name.format(epoch), dpi=200)\n",
        "    # plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def loss_vis(self):\n",
        "    plt.plot(self.g_history, 'b', self.d_history, 'r')\n",
        "    plt.title('blue: g  red: d')\n",
        "    plt.savefig('drive/My Drive/PatchWGAN/loss/gan_loss.png')\n",
        "    plt.close('all')\n",
        "\n",
        "  def update_loss_history(self):\n",
        "    self.d_history.append(sum(self.d_temp)/len(self.d_temp))\n",
        "    self.g_history.append(sum(self.g_temp)/len(self.g_temp))\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "  def __call__(self):\n",
        "    sample_noise =tf.random.uniform([24, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    image_loader = self.image_preparation(self.train_filenames, self.batch_size, self.train_steps)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "    [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "    for epoch in range(self.start_epoch, self.train_epochs+1):\n",
        "      print ('\\nepochs {}'.format(epoch))\n",
        "      imgs_ds = next(image_loader)\n",
        "\n",
        "      for steps, imgs in enumerate(imgs_ds):\n",
        "        print(\"\\r\" + 'steps{}'.format(steps+1), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.d_train(imgs)\n",
        "        [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "        if steps % self.n_critics == 0:\n",
        "          self.g_train(imgs)\n",
        "        \n",
        "      self.update_loss_history()\n",
        "                               \n",
        "      if epoch % self.image_epochs == 0:\n",
        "        self.visualise_batch(sample_noise, epoch)\n",
        "        self.loss_vis()\n",
        "\n",
        "      if epoch % self.checkpoint_epochs == 0:\n",
        "        print ('\\nSaving checkpoint at epoch{}\\n\\n'.format(epoch))\n",
        "        self.manager.save()\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  a = WGAN(img_size = 64,\n",
        "           code_num = 8,\n",
        "           batch_size = 256,\n",
        "           train_epochs = 10000, \n",
        "           train_steps = 8, \n",
        "           checkpoint_epochs = 100, \n",
        "           image_epochs = 10, \n",
        "           start_epoch = 1,\n",
        "           optimizer = RMSprop(lr=2E-6),\n",
        "           n_critics = 1\n",
        "           )\n",
        "  a()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epochs 1\n",
            "steps8\n",
            "epochs 2\n",
            "steps8\n",
            "epochs 3\n",
            "steps8\n",
            "epochs 4\n",
            "steps8\n",
            "epochs 5\n",
            "steps8\n",
            "epochs 6\n",
            "steps8\n",
            "epochs 7\n",
            "steps8\n",
            "epochs 8\n",
            "steps8\n",
            "epochs 9\n",
            "steps8\n",
            "epochs 10\n",
            "steps8\n",
            "epochs 11\n",
            "steps8\n",
            "epochs 12\n",
            "steps8\n",
            "epochs 13\n",
            "steps8\n",
            "epochs 14\n",
            "steps8\n",
            "epochs 15\n",
            "steps8\n",
            "epochs 16\n",
            "steps8\n",
            "epochs 17\n",
            "steps8\n",
            "epochs 18\n",
            "steps8\n",
            "epochs 19\n",
            "steps8\n",
            "epochs 20\n",
            "steps8\n",
            "epochs 21\n",
            "steps8\n",
            "epochs 22\n",
            "steps8\n",
            "epochs 23\n",
            "steps8\n",
            "epochs 24\n",
            "steps8\n",
            "epochs 25\n",
            "steps8\n",
            "epochs 26\n",
            "steps8\n",
            "epochs 27\n",
            "steps8\n",
            "epochs 28\n",
            "steps8\n",
            "epochs 29\n",
            "steps8\n",
            "epochs 30\n",
            "steps8\n",
            "epochs 31\n",
            "steps8\n",
            "epochs 32\n",
            "steps8\n",
            "epochs 33\n",
            "steps8\n",
            "epochs 34\n",
            "steps8\n",
            "epochs 35\n",
            "steps8\n",
            "epochs 36\n",
            "steps8\n",
            "epochs 37\n",
            "steps8\n",
            "epochs 38\n",
            "steps8\n",
            "epochs 39\n",
            "steps8\n",
            "epochs 40\n",
            "steps8\n",
            "epochs 41\n",
            "steps8\n",
            "epochs 42\n",
            "steps8\n",
            "epochs 43\n",
            "steps8\n",
            "epochs 44\n",
            "steps8\n",
            "epochs 45\n",
            "steps8\n",
            "epochs 46\n",
            "steps8\n",
            "epochs 47\n",
            "steps8\n",
            "epochs 48\n",
            "steps8\n",
            "epochs 49\n",
            "steps8\n",
            "epochs 50\n",
            "steps8\n",
            "epochs 51\n",
            "steps8\n",
            "epochs 52\n",
            "steps8\n",
            "epochs 53\n",
            "steps8\n",
            "epochs 54\n",
            "steps8\n",
            "epochs 55\n",
            "steps8\n",
            "epochs 56\n",
            "steps8\n",
            "epochs 57\n",
            "steps8\n",
            "epochs 58\n",
            "steps8\n",
            "epochs 59\n",
            "steps8\n",
            "epochs 60\n",
            "steps8\n",
            "epochs 61\n",
            "steps8\n",
            "epochs 62\n",
            "steps8\n",
            "epochs 63\n",
            "steps8\n",
            "epochs 64\n",
            "steps8\n",
            "epochs 65\n",
            "steps8\n",
            "epochs 66\n",
            "steps8\n",
            "epochs 67\n",
            "steps8\n",
            "epochs 68\n",
            "steps8\n",
            "epochs 69\n",
            "steps8\n",
            "epochs 70\n",
            "steps8\n",
            "epochs 71\n",
            "steps8\n",
            "epochs 72\n",
            "steps8\n",
            "epochs 73\n",
            "steps8\n",
            "epochs 74\n",
            "steps8\n",
            "epochs 75\n",
            "steps8\n",
            "epochs 76\n",
            "steps8\n",
            "epochs 77\n",
            "steps8\n",
            "epochs 78\n",
            "steps8\n",
            "epochs 79\n",
            "steps8\n",
            "epochs 80\n",
            "steps8\n",
            "epochs 81\n",
            "steps8\n",
            "epochs 82\n",
            "steps8\n",
            "epochs 83\n",
            "steps8\n",
            "epochs 84\n",
            "steps8\n",
            "epochs 85\n",
            "steps8\n",
            "epochs 86\n",
            "steps8\n",
            "epochs 87\n",
            "steps8\n",
            "epochs 88\n",
            "steps8\n",
            "epochs 89\n",
            "steps8\n",
            "epochs 90\n",
            "steps8\n",
            "epochs 91\n",
            "steps8\n",
            "epochs 92\n",
            "steps8\n",
            "epochs 93\n",
            "steps8\n",
            "epochs 94\n",
            "steps8\n",
            "epochs 95\n",
            "steps8\n",
            "epochs 96\n",
            "steps8\n",
            "epochs 97\n",
            "steps8\n",
            "epochs 98\n",
            "steps8\n",
            "epochs 99\n",
            "steps8\n",
            "epochs 100\n",
            "steps8\n",
            "Saving checkpoint at epoch100\n",
            "\n",
            "\n",
            "\n",
            "epochs 101\n",
            "steps8\n",
            "epochs 102\n",
            "steps8\n",
            "epochs 103\n",
            "steps8\n",
            "epochs 104\n",
            "steps8\n",
            "epochs 105\n",
            "steps8\n",
            "epochs 106\n",
            "steps8\n",
            "epochs 107\n",
            "steps8\n",
            "epochs 108\n",
            "steps8\n",
            "epochs 109\n",
            "steps8\n",
            "epochs 110\n",
            "steps8\n",
            "epochs 111\n",
            "steps8\n",
            "epochs 112\n",
            "steps8\n",
            "epochs 113\n",
            "steps8\n",
            "epochs 114\n",
            "steps8\n",
            "epochs 115\n",
            "steps8\n",
            "epochs 116\n",
            "steps8\n",
            "epochs 117\n",
            "steps8\n",
            "epochs 118\n",
            "steps8\n",
            "epochs 119\n",
            "steps8\n",
            "epochs 120\n",
            "steps8\n",
            "epochs 121\n",
            "steps8\n",
            "epochs 122\n",
            "steps8\n",
            "epochs 123\n",
            "steps8\n",
            "epochs 124\n",
            "steps8\n",
            "epochs 125\n",
            "steps8\n",
            "epochs 126\n",
            "steps8\n",
            "epochs 127\n",
            "steps8\n",
            "epochs 128\n",
            "steps8\n",
            "epochs 129\n",
            "steps8\n",
            "epochs 130\n",
            "steps8\n",
            "epochs 131\n",
            "steps8\n",
            "epochs 132\n",
            "steps8\n",
            "epochs 133\n",
            "steps8\n",
            "epochs 134\n",
            "steps8\n",
            "epochs 135\n",
            "steps8\n",
            "epochs 136\n",
            "steps8\n",
            "epochs 137\n",
            "steps8\n",
            "epochs 138\n",
            "steps8\n",
            "epochs 139\n",
            "steps8\n",
            "epochs 140\n",
            "steps8\n",
            "epochs 141\n",
            "steps8\n",
            "epochs 142\n",
            "steps8\n",
            "epochs 143\n",
            "steps8\n",
            "epochs 144\n",
            "steps8\n",
            "epochs 145\n",
            "steps8\n",
            "epochs 146\n",
            "steps8\n",
            "epochs 147\n",
            "steps8\n",
            "epochs 148\n",
            "steps8\n",
            "epochs 149\n",
            "steps8\n",
            "epochs 150\n",
            "steps8\n",
            "epochs 151\n",
            "steps8\n",
            "epochs 152\n",
            "steps8\n",
            "epochs 153\n",
            "steps8\n",
            "epochs 154\n",
            "steps8\n",
            "epochs 155\n",
            "steps8\n",
            "epochs 156\n",
            "steps8\n",
            "epochs 157\n",
            "steps8\n",
            "epochs 158\n",
            "steps8\n",
            "epochs 159\n",
            "steps8\n",
            "epochs 160\n",
            "steps8\n",
            "epochs 161\n",
            "steps8\n",
            "epochs 162\n",
            "steps8\n",
            "epochs 163\n",
            "steps8\n",
            "epochs 164\n",
            "steps8\n",
            "epochs 165\n",
            "steps8\n",
            "epochs 166\n",
            "steps8\n",
            "epochs 167\n",
            "steps8\n",
            "epochs 168\n",
            "steps8\n",
            "epochs 169\n",
            "steps8\n",
            "epochs 170\n",
            "steps8\n",
            "epochs 171\n",
            "steps8\n",
            "epochs 172\n",
            "steps8\n",
            "epochs 173\n",
            "steps8\n",
            "epochs 174\n",
            "steps8\n",
            "epochs 175\n",
            "steps8\n",
            "epochs 176\n",
            "steps8\n",
            "epochs 177\n",
            "steps8\n",
            "epochs 178\n",
            "steps8\n",
            "epochs 179\n",
            "steps8\n",
            "epochs 180\n",
            "steps8\n",
            "epochs 181\n",
            "steps8\n",
            "epochs 182\n",
            "steps8\n",
            "epochs 183\n",
            "steps8\n",
            "epochs 184\n",
            "steps8\n",
            "epochs 185\n",
            "steps8\n",
            "epochs 186\n",
            "steps8\n",
            "epochs 187\n",
            "steps8\n",
            "epochs 188\n",
            "steps8\n",
            "epochs 189\n",
            "steps8\n",
            "epochs 190\n",
            "steps8\n",
            "epochs 191\n",
            "steps8\n",
            "epochs 192\n",
            "steps8\n",
            "epochs 193\n",
            "steps8\n",
            "epochs 194\n",
            "steps8\n",
            "epochs 195\n",
            "steps8\n",
            "epochs 196\n",
            "steps8\n",
            "epochs 197\n",
            "steps8\n",
            "epochs 198\n",
            "steps8\n",
            "epochs 199\n",
            "steps8\n",
            "epochs 200\n",
            "steps8\n",
            "Saving checkpoint at epoch200\n",
            "\n",
            "\n",
            "\n",
            "epochs 201\n",
            "steps8\n",
            "epochs 202\n",
            "steps8\n",
            "epochs 203\n",
            "steps8\n",
            "epochs 204\n",
            "steps8\n",
            "epochs 205\n",
            "steps8\n",
            "epochs 206\n",
            "steps8\n",
            "epochs 207\n",
            "steps8\n",
            "epochs 208\n",
            "steps8\n",
            "epochs 209\n",
            "steps8\n",
            "epochs 210\n",
            "steps8\n",
            "epochs 211\n",
            "steps8\n",
            "epochs 212\n",
            "steps8\n",
            "epochs 213\n",
            "steps8\n",
            "epochs 214\n",
            "steps8\n",
            "epochs 215\n",
            "steps8\n",
            "epochs 216\n",
            "steps8\n",
            "epochs 217\n",
            "steps8\n",
            "epochs 218\n",
            "steps8\n",
            "epochs 219\n",
            "steps8\n",
            "epochs 220\n",
            "steps8\n",
            "epochs 221\n",
            "steps8\n",
            "epochs 222\n",
            "steps8\n",
            "epochs 223\n",
            "steps8\n",
            "epochs 224\n",
            "steps8\n",
            "epochs 225\n",
            "steps8\n",
            "epochs 226\n",
            "steps8\n",
            "epochs 227\n",
            "steps8\n",
            "epochs 228\n",
            "steps8\n",
            "epochs 229\n",
            "steps8\n",
            "epochs 230\n",
            "steps8\n",
            "epochs 231\n",
            "steps8\n",
            "epochs 232\n",
            "steps8\n",
            "epochs 233\n",
            "steps8\n",
            "epochs 234\n",
            "steps8\n",
            "epochs 235\n",
            "steps8\n",
            "epochs 236\n",
            "steps8\n",
            "epochs 237\n",
            "steps8\n",
            "epochs 238\n",
            "steps8\n",
            "epochs 239\n",
            "steps8\n",
            "epochs 240\n",
            "steps8\n",
            "epochs 241\n",
            "steps8\n",
            "epochs 242\n",
            "steps8\n",
            "epochs 243\n",
            "steps8\n",
            "epochs 244\n",
            "steps8\n",
            "epochs 245\n",
            "steps8\n",
            "epochs 246\n",
            "steps8\n",
            "epochs 247\n",
            "steps8\n",
            "epochs 248\n",
            "steps8\n",
            "epochs 249\n",
            "steps8\n",
            "epochs 250\n",
            "steps8\n",
            "epochs 251\n",
            "steps8\n",
            "epochs 252\n",
            "steps8\n",
            "epochs 253\n",
            "steps8\n",
            "epochs 254\n",
            "steps8\n",
            "epochs 255\n",
            "steps8\n",
            "epochs 256\n",
            "steps8\n",
            "epochs 257\n",
            "steps8\n",
            "epochs 258\n",
            "steps8\n",
            "epochs 259\n",
            "steps8\n",
            "epochs 260\n",
            "steps8\n",
            "epochs 261\n",
            "steps8\n",
            "epochs 262\n",
            "steps8\n",
            "epochs 263\n",
            "steps8\n",
            "epochs 264\n",
            "steps8\n",
            "epochs 265\n",
            "steps8\n",
            "epochs 266\n",
            "steps8\n",
            "epochs 267\n",
            "steps8\n",
            "epochs 268\n",
            "steps8\n",
            "epochs 269\n",
            "steps8\n",
            "epochs 270\n",
            "steps8\n",
            "epochs 271\n",
            "steps8\n",
            "epochs 272\n",
            "steps8\n",
            "epochs 273\n",
            "steps8\n",
            "epochs 274\n",
            "steps8\n",
            "epochs 275\n",
            "steps8\n",
            "epochs 276\n",
            "steps8\n",
            "epochs 277\n",
            "steps8\n",
            "epochs 278\n",
            "steps8\n",
            "epochs 279\n",
            "steps8\n",
            "epochs 280\n",
            "steps8\n",
            "epochs 281\n",
            "steps8\n",
            "epochs 282\n",
            "steps8\n",
            "epochs 283\n",
            "steps8\n",
            "epochs 284\n",
            "steps8\n",
            "epochs 285\n",
            "steps8\n",
            "epochs 286\n",
            "steps8\n",
            "epochs 287\n",
            "steps8\n",
            "epochs 288\n",
            "steps8\n",
            "epochs 289\n",
            "steps8\n",
            "epochs 290\n",
            "steps8\n",
            "epochs 291\n",
            "steps8\n",
            "epochs 292\n",
            "steps8\n",
            "epochs 293\n",
            "steps8\n",
            "epochs 294\n",
            "steps8\n",
            "epochs 295\n",
            "steps8\n",
            "epochs 296\n",
            "steps8\n",
            "epochs 297\n",
            "steps8\n",
            "epochs 298\n",
            "steps8\n",
            "epochs 299\n",
            "steps8\n",
            "epochs 300\n",
            "steps8\n",
            "Saving checkpoint at epoch300\n",
            "\n",
            "\n",
            "\n",
            "epochs 301\n",
            "steps8\n",
            "epochs 302\n",
            "steps8\n",
            "epochs 303\n",
            "steps8\n",
            "epochs 304\n",
            "steps8\n",
            "epochs 305\n",
            "steps8\n",
            "epochs 306\n",
            "steps8\n",
            "epochs 307\n",
            "steps8\n",
            "epochs 308\n",
            "steps8\n",
            "epochs 309\n",
            "steps8\n",
            "epochs 310\n",
            "steps8\n",
            "epochs 311\n",
            "steps8\n",
            "epochs 312\n",
            "steps8\n",
            "epochs 313\n",
            "steps8\n",
            "epochs 314\n",
            "steps8\n",
            "epochs 315\n",
            "steps8\n",
            "epochs 316\n",
            "steps8\n",
            "epochs 317\n",
            "steps8\n",
            "epochs 318\n",
            "steps8\n",
            "epochs 319\n",
            "steps8\n",
            "epochs 320\n",
            "steps8\n",
            "epochs 321\n",
            "steps8\n",
            "epochs 322\n",
            "steps8\n",
            "epochs 323\n",
            "steps8\n",
            "epochs 324\n",
            "steps8\n",
            "epochs 325\n",
            "steps8\n",
            "epochs 326\n",
            "steps8\n",
            "epochs 327\n",
            "steps8\n",
            "epochs 328\n",
            "steps8\n",
            "epochs 329\n",
            "steps8\n",
            "epochs 330\n",
            "steps8\n",
            "epochs 331\n",
            "steps8\n",
            "epochs 332\n",
            "steps8\n",
            "epochs 333\n",
            "steps8\n",
            "epochs 334\n",
            "steps8\n",
            "epochs 335\n",
            "steps8\n",
            "epochs 336\n",
            "steps8\n",
            "epochs 337\n",
            "steps8\n",
            "epochs 338\n",
            "steps8\n",
            "epochs 339\n",
            "steps8\n",
            "epochs 340\n",
            "steps8\n",
            "epochs 341\n",
            "steps8\n",
            "epochs 342\n",
            "steps8\n",
            "epochs 343\n",
            "steps8\n",
            "epochs 344\n",
            "steps8\n",
            "epochs 345\n",
            "steps8\n",
            "epochs 346\n",
            "steps8\n",
            "epochs 347\n",
            "steps8\n",
            "epochs 348\n",
            "steps8\n",
            "epochs 349\n",
            "steps8\n",
            "epochs 350\n",
            "steps8\n",
            "epochs 351\n",
            "steps8\n",
            "epochs 352\n",
            "steps8\n",
            "epochs 353\n",
            "steps8\n",
            "epochs 354\n",
            "steps8\n",
            "epochs 355\n",
            "steps8\n",
            "epochs 356\n",
            "steps8\n",
            "epochs 357\n",
            "steps8\n",
            "epochs 358\n",
            "steps8\n",
            "epochs 359\n",
            "steps8\n",
            "epochs 360\n",
            "steps8\n",
            "epochs 361\n",
            "steps8\n",
            "epochs 362\n",
            "steps8\n",
            "epochs 363\n",
            "steps8\n",
            "epochs 364\n",
            "steps8\n",
            "epochs 365\n",
            "steps8\n",
            "epochs 366\n",
            "steps8\n",
            "epochs 367\n",
            "steps8\n",
            "epochs 368\n",
            "steps8\n",
            "epochs 369\n",
            "steps8\n",
            "epochs 370\n",
            "steps8\n",
            "epochs 371\n",
            "steps8\n",
            "epochs 372\n",
            "steps8\n",
            "epochs 373\n",
            "steps8\n",
            "epochs 374\n",
            "steps8\n",
            "epochs 375\n",
            "steps8\n",
            "epochs 376\n",
            "steps8\n",
            "epochs 377\n",
            "steps8\n",
            "epochs 378\n",
            "steps8\n",
            "epochs 379\n",
            "steps8\n",
            "epochs 380\n",
            "steps8\n",
            "epochs 381\n",
            "steps8\n",
            "epochs 382\n",
            "steps8\n",
            "epochs 383\n",
            "steps8\n",
            "epochs 384\n",
            "steps8\n",
            "epochs 385\n",
            "steps8\n",
            "epochs 386\n",
            "steps8\n",
            "epochs 387\n",
            "steps8\n",
            "epochs 388\n",
            "steps8\n",
            "epochs 389\n",
            "steps8\n",
            "epochs 390\n",
            "steps8\n",
            "epochs 391\n",
            "steps8\n",
            "epochs 392\n",
            "steps8\n",
            "epochs 393\n",
            "steps8\n",
            "epochs 394\n",
            "steps8\n",
            "epochs 395\n",
            "steps8\n",
            "epochs 396\n",
            "steps8\n",
            "epochs 397\n",
            "steps8\n",
            "epochs 398\n",
            "steps8\n",
            "epochs 399\n",
            "steps8\n",
            "epochs 400\n",
            "steps8\n",
            "Saving checkpoint at epoch400\n",
            "\n",
            "\n",
            "\n",
            "epochs 401\n",
            "steps8\n",
            "epochs 402\n",
            "steps8\n",
            "epochs 403\n",
            "steps8\n",
            "epochs 404\n",
            "steps8\n",
            "epochs 405\n",
            "steps8\n",
            "epochs 406\n",
            "steps8\n",
            "epochs 407\n",
            "steps8\n",
            "epochs 408\n",
            "steps8\n",
            "epochs 409\n",
            "steps8\n",
            "epochs 410\n",
            "steps8\n",
            "epochs 411\n",
            "steps8\n",
            "epochs 412\n",
            "steps8\n",
            "epochs 413\n",
            "steps8\n",
            "epochs 414\n",
            "steps8\n",
            "epochs 415\n",
            "steps8\n",
            "epochs 416\n",
            "steps8\n",
            "epochs 417\n",
            "steps8\n",
            "epochs 418\n",
            "steps8\n",
            "epochs 419\n",
            "steps8\n",
            "epochs 420\n",
            "steps8\n",
            "epochs 421\n",
            "steps8\n",
            "epochs 422\n",
            "steps8\n",
            "epochs 423\n",
            "steps8\n",
            "epochs 424\n",
            "steps8\n",
            "epochs 425\n",
            "steps8\n",
            "epochs 426\n",
            "steps8\n",
            "epochs 427\n",
            "steps8\n",
            "epochs 428\n",
            "steps8\n",
            "epochs 429\n",
            "steps8\n",
            "epochs 430\n",
            "steps8\n",
            "epochs 431\n",
            "steps8\n",
            "epochs 432\n",
            "steps8\n",
            "epochs 433\n",
            "steps8\n",
            "epochs 434\n",
            "steps8\n",
            "epochs 435\n",
            "steps8\n",
            "epochs 436\n",
            "steps8\n",
            "epochs 437\n",
            "steps8\n",
            "epochs 438\n",
            "steps8\n",
            "epochs 439\n",
            "steps8\n",
            "epochs 440\n",
            "steps8\n",
            "epochs 441\n",
            "steps8\n",
            "epochs 442\n",
            "steps8\n",
            "epochs 443\n",
            "steps8\n",
            "epochs 444\n",
            "steps8\n",
            "epochs 445\n",
            "steps8\n",
            "epochs 446\n",
            "steps8\n",
            "epochs 447\n",
            "steps8\n",
            "epochs 448\n",
            "steps6"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}