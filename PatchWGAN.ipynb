{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PatchWGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Yamaguch/PatchWGAN/blob/master/PatchWGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Tzp4G4_cKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ff63193-1413-4a84-c751-dae622ec6230"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkBM0L4_c9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, \\\n",
        "MaxPooling2D, Activation, ReLU, LeakyReLU, UpSampling2D, BatchNormalization, \\\n",
        "Dropout, Dense, Flatten, Add, LayerNormalization, GaussianNoise, Reshape, Lambda\n",
        "from keras.regularizers import l2\n",
        "\n",
        "class conv_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.up = UpSampling2D((2,2))\n",
        "    self.noise = GaussianNoise(0.2)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.up(x)\n",
        "    x = self.noise(x)\n",
        "    return x\n",
        "\n",
        "class res_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(res_block, self).__init__()\n",
        "    self.conv1 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.conv2 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm1 = BatchNormalization(trainable=True)\n",
        "    self.norm2 = BatchNormalization(trainable=True)\n",
        "    self.act1 = LeakyReLU()\n",
        "    self.act2 = LeakyReLU()\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.conv1(x)\n",
        "    y = self.norm1(y)\n",
        "    y = self.act1(y)\n",
        "    y = self.conv2(y)\n",
        "    y = self.norm2(y)\n",
        "    y = self.act2(y)\n",
        "    x = self.add([x, y])\n",
        "    return x\n",
        "\n",
        "class disc_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(disc_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    # x = self.norm(x) dにnorm入れないほうがいいという噂\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class dense_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(dense_block, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "class dense_block_wo_norm(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(dense_block_wo_norm, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.act = LeakyReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2s6XE9_mkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "feb30f1e-d086-45d3-d0f4-76b9e0eeb546"
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 1024\n",
        "    self.layer_num = 5\n",
        "    self.res_num = 0\n",
        "    self.latent_num = 8\n",
        "    self.inputs = Input(shape=(self.latent_num)) \n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'generator'\n",
        "    self.kernel_regularizer= None\n",
        "      \n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    final_size = 4*4*self.channel_num\n",
        "    data_size = self.latent_num\n",
        "\n",
        "    while data_size*64 < final_size:\n",
        "      data_size *= 64\n",
        "      x = dense_block(data_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = dense_block(final_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Reshape((4, 4, self.channel_num))(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    \n",
        "    for n in range(self.layer_num):\n",
        "      for m in range(self.res_num):\n",
        "        x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num /= 2\n",
        "      x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    for m in range(self.res_num):\n",
        "      x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = Conv2D(3, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = x\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "g = Generator()\n",
        "g.model().summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_block (dense_block)    (None, 512)               6656      \n",
            "_________________________________________________________________\n",
            "dense_block_1 (dense_block)  (None, 16384)             8470528   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_block (conv_block)      (None, 8, 8, 512)         13109760  \n",
            "_________________________________________________________________\n",
            "conv_block_1 (conv_block)    (None, 16, 16, 256)       3278080   \n",
            "_________________________________________________________________\n",
            "conv_block_2 (conv_block)    (None, 32, 32, 128)       819840    \n",
            "_________________________________________________________________\n",
            "conv_block_3 (conv_block)    (None, 64, 64, 64)        205120    \n",
            "_________________________________________________________________\n",
            "conv_block_4 (conv_block)    (None, 128, 128, 32)      51360     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 128, 128, 3)       2403      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 25,943,747\n",
            "Trainable params: 25,907,971\n",
            "Non-trainable params: 35,776\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie8PcTw_nEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "486c416d-5d8a-4073-b897-d355a6b35ad4"
      },
      "source": [
        "class Discriminator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 16\n",
        "    self.layer_num = 4\n",
        "    self.latent_num = 8\n",
        "    self.input_shape = (128, 128, 3)\n",
        "    self.inputs = Input(shape=self.input_shape)\n",
        "    self.kernel_size = (4, 4)\n",
        "    self.name = 'discriminator'\n",
        "    self.kernel_regularizer= None\n",
        "\n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      x = disc_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "\n",
        "\n",
        "    # x1 = Dense(1)(x)\n",
        "    # x1 = Activation('tanh')(x1)\n",
        "\n",
        "    # x2 = Dense(self.latent_num)(x)\n",
        "    # x2 = Activation('sigmoid')(x2)\n",
        "\n",
        "    # outputs = [x1, x2]\n",
        "\n",
        "    # x = Flatten()(x)\n",
        "    # x = dense_block_wo_norm(64, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    # outputs = Dense(1)(x)\n",
        "\n",
        "    y = Conv2D(filter_num, self.kernel_size, padding='same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    y = LeakyReLU()(y)\n",
        "\n",
        "    y = Conv2D(1, self.kernel_size, padding='same', kernel_regularizer= self.kernel_regularizer)(y)\n",
        "    y = Flatten()(y)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = dense_block_wo_norm(64, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Dense(1)(x)\n",
        "\n",
        "    outputs = Concatenate()([x,y])\n",
        "\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "d = Discriminator()\n",
        "d.model().summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "disc_block (disc_block)         (None, 64, 64, 16)   784         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "disc_block_1 (disc_block)       (None, 32, 32, 32)   8224        disc_block[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "disc_block_2 (disc_block)       (None, 16, 16, 64)   32832       disc_block_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "disc_block_3 (disc_block)       (None, 8, 8, 128)    131200      disc_block_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 256)    524544      disc_block_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 8192)         0           disc_block_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 256)    0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_block_wo_norm (dense_bloc (None, 64)           524352      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 1)      4097        leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            65          dense_block_wo_norm[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 65)           0           dense_3[0][0]                    \n",
            "                                                                 flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,226,098\n",
            "Trainable params: 1,226,098\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTiix-n_pgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75a7a048-144b-47fb-9209-89bfdf696078"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import binary_crossentropy, MSE\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "\n",
        "class WGAN():\n",
        "  def __init__(self, \n",
        "               img_size=128, \n",
        "               code_num = 2048,\n",
        "               batch_size = 16, \n",
        "               train_epochs = 100, \n",
        "               train_steps = 8, \n",
        "               checkpoint_epochs = 25, \n",
        "               image_epochs = 1, \n",
        "               start_epoch = 1,\n",
        "               optimizer = Adam(learning_rate = 1e-4),\n",
        "               n_critics = 8\n",
        "               ):\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.train_epochs =  train_epochs\n",
        "    self.train_steps = train_steps\n",
        "    self.checkpoint_epochs = checkpoint_epochs\n",
        "    self.image_epochs = image_epochs\n",
        "    self.start_epoch = start_epoch\n",
        "    self.code_num = code_num\n",
        "    self.img_size = img_size\n",
        "    self.n_critics = n_critics\n",
        "    \n",
        "    self.gen_optimizer = optimizer\n",
        "    self.disc_optimizer = optimizer\n",
        "\n",
        "    g = Generator()\n",
        "    self.gen = g.model()\n",
        "    \n",
        "    d = Discriminator()\n",
        "    self.disc = d.model()\n",
        "\n",
        "    checkpoint_dir = \"drive/My Drive/PatchWGAN/checkpoint\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(gen_optimizer = self.gen_optimizer,\n",
        "                                     disc_optimizer = self.disc_optimizer,\n",
        "                                     gen = self.gen,\n",
        "                                     disc = self.disc,\n",
        "                                     )\n",
        "\n",
        "    self.manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "    train_image_path = 'drive/My Drive/samples/image'\n",
        "    \n",
        "    self.train_filenames = glob.glob(train_image_path + '/*.jpg') \n",
        "\n",
        "    checkpoint.restore(self.manager.latest_checkpoint)\n",
        "\n",
        "    self.g_history = []\n",
        "    self.d_history = []\n",
        "    # self.endec_history = []  \n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [self.img_size, self.img_size] )\n",
        "    image = image/255  # normalize to [0,1] range\n",
        "    return tf.cast(image, tf.float32)\n",
        "\n",
        "  def load_and_preprocess_image(self, path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return self.preprocess_image(image)\n",
        "\n",
        "  def dataset(self, paths, batch_size):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    img_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    img_ds = img_ds.batch(batch_size)\n",
        "    return img_ds\n",
        "\n",
        "  def image_preparation(self, filenames, batch_size, steps):\n",
        "    img_batch = []\n",
        "    while 1:\n",
        "      random.shuffle(filenames)\n",
        "      for path in filenames:\n",
        "        img_batch.append(path)\n",
        "        if len(img_batch) == steps*batch_size:\n",
        "          imgs = self.dataset(img_batch, batch_size)\n",
        "          img_batch = []\n",
        "          yield imgs\n",
        "\n",
        "  def discriminator_loss(self, original_outputs, generated_outputs):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(original_outputs), original_outputs)\n",
        "    generated_loss = binary_crossentropy(tf.zeros_like(generated_outputs), generated_outputs)\n",
        "    loss_d = tf.math.reduce_mean(real_loss + generated_loss)\n",
        "    return loss_d\n",
        "\n",
        "  def generator_loss(self, generated_outputs):\n",
        "    loss_g = tf.math.reduce_mean(binary_crossentropy(tf.ones_like(generated_outputs), generated_outputs))\n",
        "    return loss_g\n",
        "\n",
        "  def mse_loss(self, true, pred):\n",
        "    loss =  tf.math.reduce_mean(MSE(true, pred))\n",
        "    return loss\n",
        "\n",
        "  def wasserstein_loss(self, ori_outputs, gen_outputs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "  \n",
        "  def gan_train(self, imgs, n):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "\n",
        "      # ori_outputs, ori_styles = self.disc(imgs, training=True)\n",
        "      # gen_outputs, gen_styles = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      # gen_loss = self.generator_loss(gen_outputs)      \n",
        "      # disc_loss = self.discriminator_loss(ori_outputs, gen_outputs)\n",
        "\n",
        "      # re_gen_imgs = self.gen(ori_styles, training=True)\n",
        "\n",
        "      # endec_loss = self.mse_loss(imgs, re_gen_imgs)\n",
        "      # self.endec_history.append(endec_loss)\n",
        "      \n",
        "      disc_loss, gen_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_history.append(gen_loss)\n",
        "      self.d_history.append(disc_loss)\n",
        "\n",
        "      # endec_rate = 10*(0.98**n)\n",
        "      # g_loss = gen_loss + endec_loss * endec_rate\n",
        "      # d_loss = disc_loss + endec_loss * endec_rate\n",
        "\n",
        "      g_loss = gen_loss\n",
        "      d_loss = disc_loss\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def g_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=False)\n",
        "      gen_outputs = self.disc(gen_imgs, training=False)\n",
        "\n",
        "      _, g_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_temp.append(g_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "  def d_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=False)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      d_loss, _ = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def visualise_batch(self, s_1, epoch):\n",
        "    gen_img = self.gen(s_1)  \n",
        "    gen_img = (np.array(gen_img*255, np.uint8))\n",
        "    fig, axes = plt.subplots(4, 6)\n",
        "    for idx, img in enumerate(gen_img):\n",
        "      p, q = idx//6, idx%6\n",
        "      axes[p, q].imshow(img)\n",
        "      axes[p, q].axis('off')\n",
        "    \n",
        "    save_name = 'drive/My Drive/PatchWGAN/generated_image/'+'image_at_epoch_{:04d}.png'\n",
        "    plt.savefig(save_name.format(epoch), dpi=200)\n",
        "    # plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def loss_vis(self):\n",
        "    plt.plot(self.g_history, 'b', self.d_history, 'r')\n",
        "    plt.title('blue: g  red: d')\n",
        "    plt.savefig('drive/My Drive/PatchWGAN/loss/gan_loss.png')\n",
        "    plt.close('all')\n",
        "    # plt.plot(self.endec_history)\n",
        "    # plt.savefig('drive/My Drive/PatchWGAN/loss/endec_loss.png')\n",
        "    # plt.close('all')\n",
        "\n",
        "  def update_loss_history(self):\n",
        "    self.d_history.append(sum(self.d_temp)/len(self.d_temp))\n",
        "    self.g_history.append(sum(self.g_temp)/len(self.g_temp))\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "  def __call__(self):\n",
        "    sample_noise =tf.random.uniform([24, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    image_loader = self.image_preparation(self.train_filenames, self.batch_size, self.train_steps)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "    [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "    for epoch in range(self.start_epoch, self.train_epochs+1):\n",
        "      print ('\\nepochs {}'.format(epoch))\n",
        "      imgs_ds = next(image_loader)\n",
        "\n",
        "      for steps, imgs in enumerate(imgs_ds):\n",
        "        print(\"\\r\" + 'steps{}'.format(steps+1), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.d_train(imgs)\n",
        "        [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "        if steps % self.n_critics == 0:\n",
        "          self.g_train(imgs)\n",
        "        \n",
        "      self.update_loss_history()\n",
        "                               \n",
        "      if epoch % self.image_epochs == 0:\n",
        "        self.visualise_batch(sample_noise, epoch)\n",
        "        self.loss_vis()\n",
        "\n",
        "      if epoch % self.checkpoint_epochs == 0:\n",
        "        print ('\\nSaving checkpoint at epoch{}\\n\\n'.format(epoch))\n",
        "        self.manager.save()\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  a = WGAN(img_size = 128,\n",
        "           code_num = 8,\n",
        "           batch_size = 256,\n",
        "           train_epochs = 10000, \n",
        "           train_steps = 8, \n",
        "           checkpoint_epochs = 100, \n",
        "           image_epochs = 10, \n",
        "           start_epoch = 1,\n",
        "           optimizer = RMSprop(lr=1E-6),\n",
        "           n_critics = 1\n",
        "           )\n",
        "  a()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epochs 1\n",
            "steps8\n",
            "epochs 2\n",
            "steps8\n",
            "epochs 3\n",
            "steps8\n",
            "epochs 4\n",
            "steps8\n",
            "epochs 5\n",
            "steps8\n",
            "epochs 6\n",
            "steps8\n",
            "epochs 7\n",
            "steps8\n",
            "epochs 8\n",
            "steps8\n",
            "epochs 9\n",
            "steps8\n",
            "epochs 10\n",
            "steps8\n",
            "epochs 11\n",
            "steps8\n",
            "epochs 12\n",
            "steps8\n",
            "epochs 13\n",
            "steps8\n",
            "epochs 14\n",
            "steps8\n",
            "epochs 15\n",
            "steps8\n",
            "epochs 16\n",
            "steps8\n",
            "epochs 17\n",
            "steps8\n",
            "epochs 18\n",
            "steps8\n",
            "epochs 19\n",
            "steps8\n",
            "epochs 20\n",
            "steps8\n",
            "epochs 21\n",
            "steps8\n",
            "epochs 22\n",
            "steps8\n",
            "epochs 23\n",
            "steps8\n",
            "epochs 24\n",
            "steps8\n",
            "epochs 25\n",
            "steps8\n",
            "epochs 26\n",
            "steps8\n",
            "epochs 27\n",
            "steps8\n",
            "epochs 28\n",
            "steps8\n",
            "epochs 29\n",
            "steps8\n",
            "epochs 30\n",
            "steps8\n",
            "epochs 31\n",
            "steps8\n",
            "epochs 32\n",
            "steps8\n",
            "epochs 33\n",
            "steps8\n",
            "epochs 34\n",
            "steps8\n",
            "epochs 35\n",
            "steps8\n",
            "epochs 36\n",
            "steps8\n",
            "epochs 37\n",
            "steps8\n",
            "epochs 38\n",
            "steps8\n",
            "epochs 39\n",
            "steps8\n",
            "epochs 40\n",
            "steps8\n",
            "epochs 41\n",
            "steps8\n",
            "epochs 42\n",
            "steps8\n",
            "epochs 43\n",
            "steps8\n",
            "epochs 44\n",
            "steps8\n",
            "epochs 45\n",
            "steps8\n",
            "epochs 46\n",
            "steps8\n",
            "epochs 47\n",
            "steps8\n",
            "epochs 48\n",
            "steps8\n",
            "epochs 49\n",
            "steps8\n",
            "epochs 50\n",
            "steps8\n",
            "epochs 51\n",
            "steps8\n",
            "epochs 52\n",
            "steps8\n",
            "epochs 53\n",
            "steps8\n",
            "epochs 54\n",
            "steps8\n",
            "epochs 55\n",
            "steps8\n",
            "epochs 56\n",
            "steps8\n",
            "epochs 57\n",
            "steps8\n",
            "epochs 58\n",
            "steps8\n",
            "epochs 59\n",
            "steps8\n",
            "epochs 60\n",
            "steps8\n",
            "epochs 61\n",
            "steps8\n",
            "epochs 62\n",
            "steps8\n",
            "epochs 63\n",
            "steps8\n",
            "epochs 64\n",
            "steps8\n",
            "epochs 65\n",
            "steps8\n",
            "epochs 66\n",
            "steps8\n",
            "epochs 67\n",
            "steps8\n",
            "epochs 68\n",
            "steps8\n",
            "epochs 69\n",
            "steps8\n",
            "epochs 70\n",
            "steps8\n",
            "epochs 71\n",
            "steps8\n",
            "epochs 72\n",
            "steps8\n",
            "epochs 73\n",
            "steps8\n",
            "epochs 74\n",
            "steps8\n",
            "epochs 75\n",
            "steps8\n",
            "epochs 76\n",
            "steps8\n",
            "epochs 77\n",
            "steps8\n",
            "epochs 78\n",
            "steps8\n",
            "epochs 79\n",
            "steps8\n",
            "epochs 80\n",
            "steps8\n",
            "epochs 81\n",
            "steps8\n",
            "epochs 82\n",
            "steps8\n",
            "epochs 83\n",
            "steps8\n",
            "epochs 84\n",
            "steps8\n",
            "epochs 85\n",
            "steps8\n",
            "epochs 86\n",
            "steps8\n",
            "epochs 87\n",
            "steps8\n",
            "epochs 88\n",
            "steps8\n",
            "epochs 89\n",
            "steps8\n",
            "epochs 90\n",
            "steps8\n",
            "epochs 91\n",
            "steps8\n",
            "epochs 92\n",
            "steps8\n",
            "epochs 93\n",
            "steps8\n",
            "epochs 94\n",
            "steps8\n",
            "epochs 95\n",
            "steps8\n",
            "epochs 96\n",
            "steps8\n",
            "epochs 97\n",
            "steps8\n",
            "epochs 98\n",
            "steps8\n",
            "epochs 99\n",
            "steps8\n",
            "epochs 100\n",
            "steps8\n",
            "Saving checkpoint at epoch100\n",
            "\n",
            "\n",
            "\n",
            "epochs 101\n",
            "steps8\n",
            "epochs 102\n",
            "steps8\n",
            "epochs 103\n",
            "steps8\n",
            "epochs 104\n",
            "steps8\n",
            "epochs 105\n",
            "steps8\n",
            "epochs 106\n",
            "steps8\n",
            "epochs 107\n",
            "steps8\n",
            "epochs 108\n",
            "steps8\n",
            "epochs 109\n",
            "steps8\n",
            "epochs 110\n",
            "steps8\n",
            "epochs 111\n",
            "steps8\n",
            "epochs 112\n",
            "steps8\n",
            "epochs 113\n",
            "steps8\n",
            "epochs 114\n",
            "steps8\n",
            "epochs 115\n",
            "steps8\n",
            "epochs 116\n",
            "steps8\n",
            "epochs 117\n",
            "steps8\n",
            "epochs 118\n",
            "steps8\n",
            "epochs 119\n",
            "steps8\n",
            "epochs 120\n",
            "steps8\n",
            "epochs 121\n",
            "steps8\n",
            "epochs 122\n",
            "steps8\n",
            "epochs 123\n",
            "steps8\n",
            "epochs 124\n",
            "steps8\n",
            "epochs 125\n",
            "steps8\n",
            "epochs 126\n",
            "steps8\n",
            "epochs 127\n",
            "steps8\n",
            "epochs 128\n",
            "steps8\n",
            "epochs 129\n",
            "steps8\n",
            "epochs 130\n",
            "steps8\n",
            "epochs 131\n",
            "steps8\n",
            "epochs 132\n",
            "steps8\n",
            "epochs 133\n",
            "steps8\n",
            "epochs 134\n",
            "steps8\n",
            "epochs 135\n",
            "steps8\n",
            "epochs 136\n",
            "steps8\n",
            "epochs 137\n",
            "steps8\n",
            "epochs 138\n",
            "steps8\n",
            "epochs 139\n",
            "steps8\n",
            "epochs 140\n",
            "steps8\n",
            "epochs 141\n",
            "steps8\n",
            "epochs 142\n",
            "steps8\n",
            "epochs 143\n",
            "steps8\n",
            "epochs 144\n",
            "steps8\n",
            "epochs 145\n",
            "steps8\n",
            "epochs 146\n",
            "steps8\n",
            "epochs 147\n",
            "steps8\n",
            "epochs 148\n",
            "steps8\n",
            "epochs 149\n",
            "steps8\n",
            "epochs 150\n",
            "steps8\n",
            "epochs 151\n",
            "steps8\n",
            "epochs 152\n",
            "steps8\n",
            "epochs 153\n",
            "steps8\n",
            "epochs 154\n",
            "steps8\n",
            "epochs 155\n",
            "steps8\n",
            "epochs 156\n",
            "steps8\n",
            "epochs 157\n",
            "steps8\n",
            "epochs 158\n",
            "steps8\n",
            "epochs 159\n",
            "steps8\n",
            "epochs 160\n",
            "steps8\n",
            "epochs 161\n",
            "steps8\n",
            "epochs 162\n",
            "steps8\n",
            "epochs 163\n",
            "steps8\n",
            "epochs 164\n",
            "steps8\n",
            "epochs 165\n",
            "steps8\n",
            "epochs 166\n",
            "steps8\n",
            "epochs 167\n",
            "steps8\n",
            "epochs 168\n",
            "steps8\n",
            "epochs 169\n",
            "steps8\n",
            "epochs 170\n",
            "steps8\n",
            "epochs 171\n",
            "steps8\n",
            "epochs 172\n",
            "steps8\n",
            "epochs 173\n",
            "steps8\n",
            "epochs 174\n",
            "steps8\n",
            "epochs 175\n",
            "steps8\n",
            "epochs 176\n",
            "steps8\n",
            "epochs 177\n",
            "steps8\n",
            "epochs 178\n",
            "steps8\n",
            "epochs 179\n",
            "steps8\n",
            "epochs 180\n",
            "steps8\n",
            "epochs 181\n",
            "steps8\n",
            "epochs 182\n",
            "steps8\n",
            "epochs 183\n",
            "steps8\n",
            "epochs 184\n",
            "steps8\n",
            "epochs 185\n",
            "steps8\n",
            "epochs 186\n",
            "steps8\n",
            "epochs 187\n",
            "steps8\n",
            "epochs 188\n",
            "steps8\n",
            "epochs 189\n",
            "steps8\n",
            "epochs 190\n",
            "steps8\n",
            "epochs 191\n",
            "steps8\n",
            "epochs 192\n",
            "steps8\n",
            "epochs 193\n",
            "steps8\n",
            "epochs 194\n",
            "steps8\n",
            "epochs 195\n",
            "steps8\n",
            "epochs 196\n",
            "steps8\n",
            "epochs 197\n",
            "steps8\n",
            "epochs 198\n",
            "steps8\n",
            "epochs 199\n",
            "steps8\n",
            "epochs 200\n",
            "steps8\n",
            "Saving checkpoint at epoch200\n",
            "\n",
            "\n",
            "\n",
            "epochs 201\n",
            "steps8\n",
            "epochs 202\n",
            "steps8\n",
            "epochs 203\n",
            "steps8\n",
            "epochs 204\n",
            "steps8\n",
            "epochs 205\n",
            "steps8\n",
            "epochs 206\n",
            "steps8\n",
            "epochs 207\n",
            "steps8\n",
            "epochs 208\n",
            "steps8\n",
            "epochs 209\n",
            "steps8\n",
            "epochs 210\n",
            "steps8\n",
            "epochs 211\n",
            "steps8\n",
            "epochs 212\n",
            "steps8\n",
            "epochs 213\n",
            "steps8\n",
            "epochs 214\n",
            "steps8\n",
            "epochs 215\n",
            "steps8\n",
            "epochs 216\n",
            "steps8\n",
            "epochs 217\n",
            "steps8\n",
            "epochs 218\n",
            "steps8\n",
            "epochs 219\n",
            "steps8\n",
            "epochs 220\n",
            "steps8\n",
            "epochs 221\n",
            "steps8\n",
            "epochs 222\n",
            "steps8\n",
            "epochs 223\n",
            "steps8\n",
            "epochs 224\n",
            "steps8\n",
            "epochs 225\n",
            "steps8\n",
            "epochs 226\n",
            "steps8\n",
            "epochs 227\n",
            "steps8\n",
            "epochs 228\n",
            "steps8\n",
            "epochs 229\n",
            "steps8\n",
            "epochs 230\n",
            "steps8\n",
            "epochs 231\n",
            "steps8\n",
            "epochs 232\n",
            "steps8\n",
            "epochs 233\n",
            "steps8\n",
            "epochs 234\n",
            "steps8\n",
            "epochs 235\n",
            "steps8\n",
            "epochs 236\n",
            "steps8\n",
            "epochs 237\n",
            "steps8\n",
            "epochs 238\n",
            "steps8\n",
            "epochs 239\n",
            "steps8\n",
            "epochs 240\n",
            "steps8\n",
            "epochs 241\n",
            "steps8\n",
            "epochs 242\n",
            "steps8\n",
            "epochs 243\n",
            "steps8\n",
            "epochs 244\n",
            "steps8\n",
            "epochs 245\n",
            "steps8\n",
            "epochs 246\n",
            "steps8\n",
            "epochs 247\n",
            "steps8\n",
            "epochs 248\n",
            "steps8\n",
            "epochs 249\n",
            "steps8\n",
            "epochs 250\n",
            "steps8\n",
            "epochs 251\n",
            "steps8\n",
            "epochs 252\n",
            "steps8\n",
            "epochs 253\n",
            "steps8\n",
            "epochs 254\n",
            "steps8\n",
            "epochs 255\n",
            "steps8\n",
            "epochs 256\n",
            "steps8\n",
            "epochs 257\n",
            "steps8\n",
            "epochs 258\n",
            "steps8\n",
            "epochs 259\n",
            "steps8\n",
            "epochs 260\n",
            "steps8\n",
            "epochs 261\n",
            "steps8\n",
            "epochs 262\n",
            "steps8\n",
            "epochs 263\n",
            "steps8\n",
            "epochs 264\n",
            "steps8\n",
            "epochs 265\n",
            "steps8\n",
            "epochs 266\n",
            "steps8\n",
            "epochs 267\n",
            "steps8\n",
            "epochs 268\n",
            "steps8\n",
            "epochs 269\n",
            "steps8\n",
            "epochs 270\n",
            "steps8\n",
            "epochs 271\n",
            "steps8\n",
            "epochs 272\n",
            "steps8\n",
            "epochs 273\n",
            "steps8\n",
            "epochs 274\n",
            "steps8\n",
            "epochs 275\n",
            "steps8\n",
            "epochs 276\n",
            "steps8\n",
            "epochs 277\n",
            "steps8\n",
            "epochs 278\n",
            "steps8\n",
            "epochs 279\n",
            "steps8\n",
            "epochs 280\n",
            "steps8\n",
            "epochs 281\n",
            "steps8\n",
            "epochs 282\n",
            "steps8\n",
            "epochs 283\n",
            "steps8\n",
            "epochs 284\n",
            "steps8\n",
            "epochs 285\n",
            "steps8\n",
            "epochs 286\n",
            "steps8\n",
            "epochs 287\n",
            "steps8\n",
            "epochs 288\n",
            "steps8\n",
            "epochs 289\n",
            "steps8\n",
            "epochs 290\n",
            "steps8\n",
            "epochs 291\n",
            "steps8\n",
            "epochs 292\n",
            "steps8\n",
            "epochs 293\n",
            "steps8\n",
            "epochs 294\n",
            "steps8\n",
            "epochs 295\n",
            "steps8\n",
            "epochs 296\n",
            "steps8\n",
            "epochs 297\n",
            "steps8\n",
            "epochs 298\n",
            "steps8\n",
            "epochs 299\n",
            "steps8\n",
            "epochs 300\n",
            "steps8\n",
            "Saving checkpoint at epoch300\n",
            "\n",
            "\n",
            "\n",
            "epochs 301\n",
            "steps8\n",
            "epochs 302\n",
            "steps8\n",
            "epochs 303\n",
            "steps8\n",
            "epochs 304\n",
            "steps8\n",
            "epochs 305\n",
            "steps8\n",
            "epochs 306\n",
            "steps8\n",
            "epochs 307\n",
            "steps8\n",
            "epochs 308\n",
            "steps8\n",
            "epochs 309\n",
            "steps8\n",
            "epochs 310\n",
            "steps8\n",
            "epochs 311\n",
            "steps8\n",
            "epochs 312\n",
            "steps8\n",
            "epochs 313\n",
            "steps8\n",
            "epochs 314\n",
            "steps8\n",
            "epochs 315\n",
            "steps8\n",
            "epochs 316\n",
            "steps8\n",
            "epochs 317\n",
            "steps8\n",
            "epochs 318\n",
            "steps8\n",
            "epochs 319\n",
            "steps8\n",
            "epochs 320\n",
            "steps8\n",
            "epochs 321\n",
            "steps8\n",
            "epochs 322\n",
            "steps8\n",
            "epochs 323\n",
            "steps8\n",
            "epochs 324\n",
            "steps8\n",
            "epochs 325\n",
            "steps8\n",
            "epochs 326\n",
            "steps8\n",
            "epochs 327\n",
            "steps8\n",
            "epochs 328\n",
            "steps8\n",
            "epochs 329\n",
            "steps8\n",
            "epochs 330\n",
            "steps8\n",
            "epochs 331\n",
            "steps8\n",
            "epochs 332\n",
            "steps8\n",
            "epochs 333\n",
            "steps8\n",
            "epochs 334\n",
            "steps8\n",
            "epochs 335\n",
            "steps8\n",
            "epochs 336\n",
            "steps8\n",
            "epochs 337\n",
            "steps8\n",
            "epochs 338\n",
            "steps8\n",
            "epochs 339\n",
            "steps8\n",
            "epochs 340\n",
            "steps8\n",
            "epochs 341\n",
            "steps8\n",
            "epochs 342\n",
            "steps8\n",
            "epochs 343\n",
            "steps8\n",
            "epochs 344\n",
            "steps8\n",
            "epochs 345\n",
            "steps8\n",
            "epochs 346\n",
            "steps8\n",
            "epochs 347\n",
            "steps8\n",
            "epochs 348\n",
            "steps8\n",
            "epochs 349\n",
            "steps8\n",
            "epochs 350\n",
            "steps8\n",
            "epochs 351\n",
            "steps8\n",
            "epochs 352\n",
            "steps8\n",
            "epochs 353\n",
            "steps8\n",
            "epochs 354\n",
            "steps8\n",
            "epochs 355\n",
            "steps8\n",
            "epochs 356\n",
            "steps8\n",
            "epochs 357\n",
            "steps8\n",
            "epochs 358\n",
            "steps8\n",
            "epochs 359\n",
            "steps8\n",
            "epochs 360\n",
            "steps8\n",
            "epochs 361\n",
            "steps8\n",
            "epochs 362\n",
            "steps8\n",
            "epochs 363\n",
            "steps8\n",
            "epochs 364\n",
            "steps8\n",
            "epochs 365\n",
            "steps8\n",
            "epochs 366\n",
            "steps8\n",
            "epochs 367\n",
            "steps8\n",
            "epochs 368\n",
            "steps8\n",
            "epochs 369\n",
            "steps8\n",
            "epochs 370\n",
            "steps8\n",
            "epochs 371\n",
            "steps8\n",
            "epochs 372\n",
            "steps8\n",
            "epochs 373\n",
            "steps8\n",
            "epochs 374\n",
            "steps8\n",
            "epochs 375\n",
            "steps8\n",
            "epochs 376\n",
            "steps8\n",
            "epochs 377\n",
            "steps8\n",
            "epochs 378\n",
            "steps8\n",
            "epochs 379\n",
            "steps8\n",
            "epochs 380\n",
            "steps8\n",
            "epochs 381\n",
            "steps8\n",
            "epochs 382\n",
            "steps8\n",
            "epochs 383\n",
            "steps8\n",
            "epochs 384\n",
            "steps8\n",
            "epochs 385\n",
            "steps8\n",
            "epochs 386\n",
            "steps8\n",
            "epochs 387\n",
            "steps8\n",
            "epochs 388\n",
            "steps8\n",
            "epochs 389\n",
            "steps8\n",
            "epochs 390\n",
            "steps8\n",
            "epochs 391\n",
            "steps8\n",
            "epochs 392\n",
            "steps8\n",
            "epochs 393\n",
            "steps8\n",
            "epochs 394\n",
            "steps8\n",
            "epochs 395\n",
            "steps8\n",
            "epochs 396\n",
            "steps8\n",
            "epochs 397\n",
            "steps8\n",
            "epochs 398\n",
            "steps8\n",
            "epochs 399\n",
            "steps8\n",
            "epochs 400\n",
            "steps8\n",
            "Saving checkpoint at epoch400\n",
            "\n",
            "\n",
            "\n",
            "epochs 401\n",
            "steps8\n",
            "epochs 402\n",
            "steps8\n",
            "epochs 403\n",
            "steps8\n",
            "epochs 404\n",
            "steps8\n",
            "epochs 405\n",
            "steps8\n",
            "epochs 406\n",
            "steps8\n",
            "epochs 407\n",
            "steps8\n",
            "epochs 408\n",
            "steps8\n",
            "epochs 409\n",
            "steps8\n",
            "epochs 410\n",
            "steps8\n",
            "epochs 411\n",
            "steps8\n",
            "epochs 412\n",
            "steps8\n",
            "epochs 413\n",
            "steps8\n",
            "epochs 414\n",
            "steps8\n",
            "epochs 415\n",
            "steps8\n",
            "epochs 416\n",
            "steps8\n",
            "epochs 417\n",
            "steps8\n",
            "epochs 418\n",
            "steps8\n",
            "epochs 419\n",
            "steps8\n",
            "epochs 420\n",
            "steps8\n",
            "epochs 421\n",
            "steps8\n",
            "epochs 422\n",
            "steps8\n",
            "epochs 423\n",
            "steps8\n",
            "epochs 424\n",
            "steps8\n",
            "epochs 425\n",
            "steps8\n",
            "epochs 426\n",
            "steps8\n",
            "epochs 427\n",
            "steps8\n",
            "epochs 428\n",
            "steps8\n",
            "epochs 429\n",
            "steps8\n",
            "epochs 430\n",
            "steps8\n",
            "epochs 431\n",
            "steps8\n",
            "epochs 432\n",
            "steps8\n",
            "epochs 433\n",
            "steps8\n",
            "epochs 434\n",
            "steps8\n",
            "epochs 435\n",
            "steps8\n",
            "epochs 436\n",
            "steps8\n",
            "epochs 437\n",
            "steps8\n",
            "epochs 438\n",
            "steps8\n",
            "epochs 439\n",
            "steps8\n",
            "epochs 440\n",
            "steps8\n",
            "epochs 441\n",
            "steps8\n",
            "epochs 442\n",
            "steps8\n",
            "epochs 443\n",
            "steps8\n",
            "epochs 444\n",
            "steps8\n",
            "epochs 445\n",
            "steps8\n",
            "epochs 446\n",
            "steps8\n",
            "epochs 447\n",
            "steps8\n",
            "epochs 448\n",
            "steps8\n",
            "epochs 449\n",
            "steps8\n",
            "epochs 450\n",
            "steps8\n",
            "epochs 451\n",
            "steps8\n",
            "epochs 452\n",
            "steps8\n",
            "epochs 453\n",
            "steps8\n",
            "epochs 454\n",
            "steps8\n",
            "epochs 455\n",
            "steps8\n",
            "epochs 456\n",
            "steps8\n",
            "epochs 457\n",
            "steps8\n",
            "epochs 458\n",
            "steps8\n",
            "epochs 459\n",
            "steps8\n",
            "epochs 460\n",
            "steps8\n",
            "epochs 461\n",
            "steps8\n",
            "epochs 462\n",
            "steps8\n",
            "epochs 463\n",
            "steps8\n",
            "epochs 464\n",
            "steps8\n",
            "epochs 465\n",
            "steps8\n",
            "epochs 466\n",
            "steps8\n",
            "epochs 467\n",
            "steps8\n",
            "epochs 468\n",
            "steps8\n",
            "epochs 469\n",
            "steps8\n",
            "epochs 470\n",
            "steps8\n",
            "epochs 471\n",
            "steps8\n",
            "epochs 472\n",
            "steps8\n",
            "epochs 473\n",
            "steps8\n",
            "epochs 474\n",
            "steps8\n",
            "epochs 475\n",
            "steps8\n",
            "epochs 476\n",
            "steps8\n",
            "epochs 477\n",
            "steps8\n",
            "epochs 478\n",
            "steps8\n",
            "epochs 479\n",
            "steps8\n",
            "epochs 480\n",
            "steps8\n",
            "epochs 481\n",
            "steps8\n",
            "epochs 482\n",
            "steps8\n",
            "epochs 483\n",
            "steps8\n",
            "epochs 484\n",
            "steps8\n",
            "epochs 485\n",
            "steps8\n",
            "epochs 486\n",
            "steps8\n",
            "epochs 487\n",
            "steps8\n",
            "epochs 488\n",
            "steps8\n",
            "epochs 489\n",
            "steps8\n",
            "epochs 490\n",
            "steps8\n",
            "epochs 491\n",
            "steps8\n",
            "epochs 492\n",
            "steps8\n",
            "epochs 493\n",
            "steps8\n",
            "epochs 494\n",
            "steps8\n",
            "epochs 495\n",
            "steps8\n",
            "epochs 496\n",
            "steps8\n",
            "epochs 497\n",
            "steps8\n",
            "epochs 498\n",
            "steps8\n",
            "epochs 499\n",
            "steps8\n",
            "epochs 500\n",
            "steps8\n",
            "Saving checkpoint at epoch500\n",
            "\n",
            "\n",
            "\n",
            "epochs 501\n",
            "steps8\n",
            "epochs 502\n",
            "steps8\n",
            "epochs 503\n",
            "steps8\n",
            "epochs 504\n",
            "steps8\n",
            "epochs 505\n",
            "steps8\n",
            "epochs 506\n",
            "steps8\n",
            "epochs 507\n",
            "steps8\n",
            "epochs 508\n",
            "steps8\n",
            "epochs 509\n",
            "steps8\n",
            "epochs 510\n",
            "steps8\n",
            "epochs 511\n",
            "steps8\n",
            "epochs 512\n",
            "steps8\n",
            "epochs 513\n",
            "steps8\n",
            "epochs 514\n",
            "steps8\n",
            "epochs 515\n",
            "steps8\n",
            "epochs 516\n",
            "steps8\n",
            "epochs 517\n",
            "steps8\n",
            "epochs 518\n",
            "steps8\n",
            "epochs 519\n",
            "steps8\n",
            "epochs 520\n",
            "steps8\n",
            "epochs 521\n",
            "steps8\n",
            "epochs 522\n",
            "steps8\n",
            "epochs 523\n",
            "steps8\n",
            "epochs 524\n",
            "steps8\n",
            "epochs 525\n",
            "steps8\n",
            "epochs 526\n",
            "steps8\n",
            "epochs 527\n",
            "steps8\n",
            "epochs 528\n",
            "steps8\n",
            "epochs 529\n",
            "steps8\n",
            "epochs 530\n",
            "steps8\n",
            "epochs 531\n",
            "steps8\n",
            "epochs 532\n",
            "steps8\n",
            "epochs 533\n",
            "steps8\n",
            "epochs 534\n",
            "steps8\n",
            "epochs 535\n",
            "steps8\n",
            "epochs 536\n",
            "steps8\n",
            "epochs 537\n",
            "steps8\n",
            "epochs 538\n",
            "steps8\n",
            "epochs 539\n",
            "steps8\n",
            "epochs 540\n",
            "steps8\n",
            "epochs 541\n",
            "steps8\n",
            "epochs 542\n",
            "steps8\n",
            "epochs 543\n",
            "steps8\n",
            "epochs 544\n",
            "steps8\n",
            "epochs 545\n",
            "steps8\n",
            "epochs 546\n",
            "steps8\n",
            "epochs 547\n",
            "steps8\n",
            "epochs 548\n",
            "steps8\n",
            "epochs 549\n",
            "steps8\n",
            "epochs 550\n",
            "steps8\n",
            "epochs 551\n",
            "steps8\n",
            "epochs 552\n",
            "steps8\n",
            "epochs 553\n",
            "steps8\n",
            "epochs 554\n",
            "steps8\n",
            "epochs 555\n",
            "steps8\n",
            "epochs 556\n",
            "steps8\n",
            "epochs 557\n",
            "steps8\n",
            "epochs 558\n",
            "steps8\n",
            "epochs 559\n",
            "steps8\n",
            "epochs 560\n",
            "steps8\n",
            "epochs 561\n",
            "steps8\n",
            "epochs 562\n",
            "steps8\n",
            "epochs 563\n",
            "steps8\n",
            "epochs 564\n",
            "steps8\n",
            "epochs 565\n",
            "steps8\n",
            "epochs 566\n",
            "steps8\n",
            "epochs 567\n",
            "steps8\n",
            "epochs 568\n",
            "steps8\n",
            "epochs 569\n",
            "steps8\n",
            "epochs 570\n",
            "steps8\n",
            "epochs 571\n",
            "steps8\n",
            "epochs 572\n",
            "steps8\n",
            "epochs 573\n",
            "steps8\n",
            "epochs 574\n",
            "steps8\n",
            "epochs 575\n",
            "steps8\n",
            "epochs 576\n",
            "steps8\n",
            "epochs 577\n",
            "steps8\n",
            "epochs 578\n",
            "steps8\n",
            "epochs 579\n",
            "steps8\n",
            "epochs 580\n",
            "steps8\n",
            "epochs 581\n",
            "steps8\n",
            "epochs 582\n",
            "steps8\n",
            "epochs 583\n",
            "steps8\n",
            "epochs 584\n",
            "steps8\n",
            "epochs 585\n",
            "steps8\n",
            "epochs 586\n",
            "steps8\n",
            "epochs 587\n",
            "steps8\n",
            "epochs 588\n",
            "steps8\n",
            "epochs 589\n",
            "steps8\n",
            "epochs 590\n",
            "steps8\n",
            "epochs 591\n",
            "steps8\n",
            "epochs 592\n",
            "steps8\n",
            "epochs 593\n",
            "steps8\n",
            "epochs 594\n",
            "steps8\n",
            "epochs 595\n",
            "steps8\n",
            "epochs 596\n",
            "steps8\n",
            "epochs 597\n",
            "steps8\n",
            "epochs 598\n",
            "steps8\n",
            "epochs 599\n",
            "steps8\n",
            "epochs 600\n",
            "steps8\n",
            "Saving checkpoint at epoch600\n",
            "\n",
            "\n",
            "\n",
            "epochs 601\n",
            "steps8\n",
            "epochs 602\n",
            "steps8\n",
            "epochs 603\n",
            "steps8\n",
            "epochs 604\n",
            "steps8\n",
            "epochs 605\n",
            "steps8\n",
            "epochs 606\n",
            "steps8\n",
            "epochs 607\n",
            "steps8\n",
            "epochs 608\n",
            "steps8\n",
            "epochs 609\n",
            "steps8\n",
            "epochs 610\n",
            "steps8\n",
            "epochs 611\n",
            "steps8\n",
            "epochs 612\n",
            "steps8\n",
            "epochs 613\n",
            "steps8\n",
            "epochs 614\n",
            "steps8\n",
            "epochs 615\n",
            "steps8\n",
            "epochs 616\n",
            "steps8\n",
            "epochs 617\n",
            "steps8\n",
            "epochs 618\n",
            "steps8\n",
            "epochs 619\n",
            "steps8\n",
            "epochs 620\n",
            "steps8\n",
            "epochs 621\n",
            "steps8\n",
            "epochs 622\n",
            "steps8\n",
            "epochs 623\n",
            "steps8\n",
            "epochs 624\n",
            "steps8\n",
            "epochs 625\n",
            "steps8"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}